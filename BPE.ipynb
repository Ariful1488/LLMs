{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14320f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gmarifulislam/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032999b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "print('tiktoken version:', importlib.metadata.version('tiktoken'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7a45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get class like SimpleTokenizerV2 which we made in the last file \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2384631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 2255, 345, 588, 8887, 30, 220, 50256, 287, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello, dou you like tea? <|endoftext|> in the sunlit terraces\" \n",
    "        \" of someunknownPlace\")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special= {\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9161788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_f = tokenizer.decode(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168b6ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, dou you like tea? <|endoftext|> in the sunlit terraces of someunknownPlace'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f52208c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating input-terget Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1591a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section we implement a data loader that fetches the input-terget pairs using sliding window approach\n",
    "# To get started, we will first tokenize the whole the verdict short story, we worked with earlier BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d803e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text) #token words id\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bda5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to remove first 50 tokens from the data set for denonstration purpose which results in a slightly more interesting\n",
    "#enc_text is encoded version of our rwa_text\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "130735c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "#the context size determines how many tokens are included in the input \n",
    "context_size = 4  #length of the input\n",
    "#the context size of 4 means that the model is traiined to look at a sequence of 4 words \n",
    "# to predict the next word in the sequence.\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1: context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f0edde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ------> 4920\n",
      "[290, 4920] ------> 2241\n",
      "[290, 4920, 2241] ------> 287\n",
      "[290, 4920, 2241, 287] ------> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"------>\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5538b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"----->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9715b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62768619",
   "metadata": {},
   "source": [
    "'''\n",
    "What is a Tensor?\n",
    "A tensor is just a container for numbers (like arrays or matrices), but it can have any number of dimensions.\n",
    "1. 0D tensor (scalar): just one number : 7\n",
    "2. 1D tensor (vector): a list of numbers: [1, 2, 3]\n",
    "3. 2D tensor (matrix): rows and columns:\n",
    "                                        [[1, 2, 3],\n",
    "                                         [4, 5, 6]]\n",
    "\n",
    "\n",
    "4. 3D tensor: like a cube (stack of matrices):\n",
    "\n",
    "                        [[[1, 2], [3, 4]],\n",
    "                        \n",
    "                         [[5, 6], [7, 8]]]\n",
    "\n",
    "5. ND tensor: can go to higher dimensions (for images, videos, etc.)\n",
    "\n",
    "\n",
    "Why use tensors?\n",
    "\n",
    "They run fast on CPU and GPU.\n",
    "\n",
    "They support autograd (automatic differentiation for training).\n",
    "\n",
    "They are the universal data format for deep learning models.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653dd80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep-1: Tokenize the entire text\\nStep-2: Use a sliding window to chunk the book into overlapping sequences of max_length\\nStep-3: Return the total number of rows in the dataset\\nStep-4: Return a single row from the dataset\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for the efficient data loader implementation, we will use pytorch's built-in Dataset and DataLoader classes\n",
    "'''\n",
    "Step-1: Tokenize the entire text\n",
    "Step-2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "Step-3: Return the total number of rows in the dataset\n",
    "Step-4: Return a single row from the dataset\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f778a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk  = token_ids[i : i+max_length]\n",
    "            target_chunk = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42179a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code will use the GPTDatasetV1 to load the inputs in batches via a pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2619aba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: Initialize the tokenizer\\n\\nStep-2: Create the Dataset\\n\\nStep-3: drop_last = True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\\n\\nStep-4: The number of cpu process to use for preprocessing\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step-2: Create the Dataset\n",
    "\n",
    "Step-3: drop_last = True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "\n",
    "Step-4: The number of cpu process to use for preprocessing\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dcb08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_length = 256,\n",
    "                         stride = 128, shuffle = True, drop_last = True,\n",
    "                          num_workers = 0):\n",
    "    #Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    #crate dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    #Create Dataloader\n",
    "    # Here DataLoader will access pervious GPTDatasetV1 class by getitem method\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fdffbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nLet's test the dataloader with a batch size of 1 for an LLM with a context size of 4\\nThis will develop an intution of how the GPTDatasetV1 class and the create_dataloader_v1 work together\\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4\n",
    "This will develop an intution of how the GPTDatasetV1 class and the create_dataloader_v1 work together\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75a81ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\",encoding= \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c27008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataloader into a python iterator to fetch the next entry via Python's built-in next() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1968eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0db5d580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1807, 3619,  402,  271]]), tensor([[ 3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f03d14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Tergets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle= False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, tergets = next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"\\nTergets:\\n\", tergets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9a828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
