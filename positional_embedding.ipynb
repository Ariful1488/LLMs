{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a844e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1445f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966e94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c317881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ntoken embaddeing matrics is a look up table.\\nnow we need to create input ids, sowe can generate vector embedding or token embedding.\\nto create input ids we need to use dataloader,vwe embaded each token in each batch into a 256-dimensional\\nvector.If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 X 256 tensor.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "token embaddeing matrics is a look up table.\n",
    "now we need to create input ids, sowe can generate vector embedding or token embedding.\n",
    "to create input ids we need to use dataloader,vwe embaded each token in each batch into a 256-dimensional\n",
    "vector.If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 X 256 tensor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80ccde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's intantiate the data loader(Data sampling with a sliding window), first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec29e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.terget_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text, allowed_special= {\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i+max_length]\n",
    "            terget_chunk = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk,dtype=torch.long))\n",
    "            self.terget_ids.append(torch.tensor(terget_chunk,dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.terget_ids[idx]\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader_v1(text,batch_size = 4, max_length = 256,\n",
    "                         stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    #initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    #Craete dataset\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    #create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last= drop_last,\n",
    "        num_workers=num_workers\n",
    "\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c7369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT2 size\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39d0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8a0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1 (raw_text, batch_size= 8, max_length= max_length,stride=max_length, shuffle=False)\n",
    "data_itr = iter(dataloader)\n",
    "inputs, targets = next(data_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d49d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      " Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\n Inputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1fdecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3eb89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As we can tell based on the 8x4x256 - dimensional tensor output, each token ID is now  \\nembedded as a 256 -dimensional vector.\\n\\nFor a GPT model's absolute embedding approach, we just need to create another embedding layer\\nthat has the same dimension\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''As we can tell based on the 8x4x256 - dimensional tensor output, each token ID is now  \n",
    "embedded as a 256 -dimensional vector.\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create another embedding layer\n",
    "that has the same dimension\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78078a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context length means how many token id we are taking as input in a batch means 4\n",
    "# Since each token has 256 dimenssion so for positional embedding dimenssion will be 256\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650651b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c993d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "#we can see that our positional embedding shape is also[4,256]\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebbcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
